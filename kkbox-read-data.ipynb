{"cells":[{"cell_type":"markdown","source":["<h5> Objectives </h5>\n<ol>\n  <li> Reads data from S3\n  <li> Creates the schema\n  <li> Writes the data back to S3 as parquet files, so that it is easy to query and faster to use\n</ol>"],"metadata":{}},{"cell_type":"code","source":["#uncomment for jupyter notebook\n#import findspark\n#findspark.init()\n\n#import pyspark\n#sc = pyspark.SparkContext()\n#spark = pyspark.sql.SparkSession(sc)"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# General imports\nimport matplotlib.pyplot as pyplt\nimport numpy as np\nfrom ggplot import *\nfrom datetime import datetime\n\nimport pyspark.sql.functions as sf\nfrom pyspark.sql import Row\n"],"metadata":{"collapsed":false,"deletable":true,"editable":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["<h4> Helper functions </h4>"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["def getInt(x):\n    try: v = int(x)\n    except: v = 0\n    return v\n\n\ndef getDate(x):\n    try: v = datetime.strptime(x,\"%Y%m%d\").date()\n    except: v = datetime.strptime(\"19010101\",\"%Y%m%d\").date()\n    return v\n\ndef nothing(x):\n    return x\n\ndef getFloat(x):\n    try: v = float(x)\n    except: v = 0\n    return v\n\n\n# Rename columns of the dataframe using the array of new names\ndef renameCols(df,newNames):\n    nDF = df\n    assert(len(df.schema.names) == len(newNames))\n    for i in range(0,len(df.schema.names)):\n        nDF = nDF.withColumnRenamed(nDF.schema.names[i],newNames[i])\n    return nDF\n\n# Takes a list of functions, and applies thme to the row, \n# input: array of strings\n# output: tuple ( types as determinted by the return type of the functions )\ndef getTupleFromSchema(l,colExtractors):\n   assert(len(colExtractors) == len(l))\n   return tuple ( [colExtractors[i](l[i]) for i in range(0,len(l))] )\n"],"metadata":{"collapsed":false,"deletable":true,"editable":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":["\n# These functions generate a dataframe from a file\n# fname: Name of CSV file from which to get the dataframe\n# colExtractors: a list of functions that will be used to convert to the schemaType\n# colExtractors are assumed to handle errors\ndef getDF(fname,colExtractors):\n   # read the file\n   inRDD = sc.textFile(fname)\n\n    # get the schema from the first row\n   schema = ''.join(inRDD.take(1)).split(',')\n    \n   # leave out the first row (assumed column)\n   allRowsExceptFirst = inRDD.zipWithIndex().filter(lambda r: r[1] > 0 ).map(lambda r: r[0])\n   \n   # create the RDD of interest, mapping the colums as we go. \n   tRDD = allRowsExceptFirst.map(lambda l: l.replace(' ','').split(','))\\\n              .map(lambda l: getTupleFromSchema(l, colExtractors))\n    \n    # Rename columns based on schema\n   tDF = renameCols(tRDD.toDF(),schema)\n   return tDF\n\n\n# Exactly same as above but adds a synthetic index to be used if needed\ndef getDFWithIndex(fname,colExtractors):\n   # read the file\n   inRDD = sc.textFile(fname)\n\n    # get the schema from the first row\n   schema = [u'idx_'] + ''.join(inRDD.take(1)).split(',')\n    \n   # leave out the first row (assumed column)\n   allRowsExceptFirst = inRDD.zipWithIndex().filter(lambda r: r[1] > 0 ).map(lambda r: str(r[1]) + ',' + r[0])\n   \n   # create the RDD of interest, mapping the colums as we go. \n   tRDD = allRowsExceptFirst.map(lambda l: l.replace(' ','').split(','))\\\n              .map(lambda l: getTupleFromSchema(l, [getInt] + colExtractors))\n    \n    # Rename columns based on schema\n   tDF = renameCols(tRDD.toDF(),schema)\n   return tDF  "],"metadata":{"collapsed":false,"deletable":true,"editable":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["<h2> Create dataframes here </h2>"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["<h4> Setup data sources </h4>\n<h6> Steps to follow </h6>\n<ul> \n  <li> Upload files to S3 bucket\n  <li> Specify the configuration below and mount it\n</ul>"],"metadata":{}},{"cell_type":"code","source":["# Replace with your values\n#\n# NOTE: Set the access to this notebook appropriately to protect the security of your keys.\n# Or you can delete this cell after you run the mount command below once successfully.\n\n# Uncomment below to load your data th first time\n#ACCESS_KEY = \"REPLACE_WITH_YOUR_ACCESS_KEY\"\n#SECRET_KEY = \"REPLACE_WITH_YOUR_SECRET_KEY\"\n#ENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n#AWS_BUCKET_NAME = \"REPLACE_WITH_YOUR_S3_BUCKET\"\n#MOUNT_NAME = \"REPLACE_WITH_YOUR_MOUNT_NAME\"\n\n#dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Data files\nmembers_data           = '/mnt/kkboxmount/members.csv'\nsample_submission_zero_data = '/mnt/kkboxmount/sample_submission_zero.csv'\ntrain_data             = '/mnt/kkboxmount/train.csv'\ntransactions_data      = '/mnt/kkboxmount/transactions.csv'\nuser_logs_part_data    = '/mnt/kkboxmount/user_logs_part.csv'\nuser_logs_data         = '/mnt/kkboxmount/user_logs.csv.gz'\n\n# Parquet files\nmembers_par           = '/mnt/kkboxmount/parq/members'\nsample_submission_zero_par = '/mnt/kkboxmount/parq/sample_submission_zero'\ntrain_par             = '/mnt/kkboxmount/parq/train'\ntransactions_par      = '/mnt/kkboxmount/parq/transactions'\nuser_logs_part_par    = '/mnt/kkboxmount/parq/user_logs_part'\nuser_logs_par         = '/mnt/kkboxmount/parq/user_logs'\n\n# Tables files\nmembers_table           = 'unmgmt_members_table'\nsample_submission_zero_table = 'unmgmt_sample_submission_zero_table'\ntrain_table             = 'unmgmt_train_table'\ntransactions_table      = 'unmgmt_transactions_table'\nuser_logs_part_table    = 'unmgmt_user_logs_part_table'\nuser_logs_1_table         = 'unmgmt_user_logs_1_table'\nuser_logs_2_table         = 'unmgmt_user_logs_2_table'\nuser_logs_3_table         = 'unmgmt_user_logs_2_table'"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["<h5> Get a clean memDF (\"DF of members\") </h5>"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["sc.textFile(members_data).take(2)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["memDF = getDF(members_data,[nothing,getInt,getInt,nothing,getInt,getDate,getDate]).repartition(\"msno\",\"registration_init_time\",\"expiration_date\")\nmemDF.show(3)"],"metadata":{"collapsed":false,"deletable":true,"editable":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["<h5> Get a clean sampleSubZeroDF (\"DF of Sample Submission\") </h5>"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["sampleSubZeroDF = getDF(sample_submission_zero_data,[nothing,getInt]).repartition(\"msno\")\nsampleSubZeroDF.show(5)"],"metadata":{"collapsed":false,"deletable":true,"editable":true},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["<h5> Get a clean trainDF (\"DF of train\") </h5>"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["trainDF = getDF(train_data,[nothing,getInt]).repartition(\"msno\")\ntrainDF.show(5)"],"metadata":{"collapsed":false,"deletable":true,"editable":true},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["<h5> Get a clean transactionsDF (\"DF of transactions\") </h5>"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["transactionsDF = getDF(transactions_data,[nothing,getInt,getInt,getInt,getInt,getInt,getDate,getDate,getInt]).repartition(\"msno\")\ntransactionsDF.show(3)"],"metadata":{"collapsed":false,"deletable":true,"editable":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":["userLogPartDF = getDF(user_logs_part_data,[nothing,getDate,getInt,getInt,getInt,getInt,getInt,getInt,getFloat]).repartition(\"msno\")\nuserLogPartDF.show(3)"],"metadata":{"collapsed":false,"deletable":true,"editable":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#User Logs is really large so we are going to split it into three dataframes\n# Step1: Add an idx\n# Step2: Create three dataframes what we will write (idx, msno, date), (idx, num_25,num_50,num_75), (idx, num_100, num_unq,total_secs)\n# Step3: Write them to S3\n"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["userLogAllDF = getDF(user_logs_data,[nothing,getDate,getInt,getInt,getInt,getInt,getInt,getInt,getFloat]).repartition(\"msno\")\nuserLogAllDF.show(3)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["#userLog_1 = userLogAllDF.select(['idx_','msno','date']).repartition(\"msno\",\"idx_\")\n#userLog_2 = userLogAllDF.select(['idx_','msno','num_25','num_50','num_75']).repartition(\"msno\",\"idx_\")\n#userLog_3 = userLogAllDF.select(['idx_','msno','num_100','num_unq','total_secs']).repartition(\"msno\",\"idx_\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["<h3> Save all these datasets as parquet files and Tables so queries are faster </h3>"],"metadata":{}},{"cell_type":"code","source":["memDF.write.parquet(members_par,mode=\"overwrite\",partitionBy=\"msno\",compression=\"snappy\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["memDF.write.saveAsTable(members_table)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["sampleSubZeroDF.write.parquet(sample_submission_zero_par,mode=\"overwrite\",partitionBy=\"msno\",compression=\"snappy\")\ntrainDF.write.parquet(train_par,mode=\"overwrite\",partitionBy=\"msno\",compression=\"snappy\")\nuserLogPartDF.write.parquet(user_logs_part_par,mode=\"overwrite\",partitionBy=\"msno\",compression=\"snappy\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["sampleSubZeroDF.write.saveAsTable(sample_submission_zero_table)\ntrainDF.write.saveAsTable(train_table)\nuserLogPartDF.write.saveAsTable(user_logs_part_table)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["transactionsDF.write.parquet(transactions_par,mode=\"overwrite\",partitionBy=\"msno\",compression=\"snappy\")"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["transactionsDF.write.saveAsTable(transactions_table)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["userLogAllDF.write.parquet(user_logs_par,mode=\"overwrite\",partitionBy=\"msno\",compression=\"snappy\")\n#userLog_1.write.parquet(user_logs_par,mode=\"overwrite\",partitionBy=\"msno\",compression=\"snappy\")\n#userLog_2.write.parquet(user_logs_par,mode=\"overwrite\",partitionBy=\"msno\",compression=\"snappy\")\n#userLog_3.write.parquet(user_logs_par,mode=\"overwrite\",partitionBy=\"msno\",compression=\"snappy\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["#userLog_1.write.saveAsTable(user_logs_1_table)\n#userLog_2.write.saveAsTable(user_logs_2_table)\n#userLog_3.write.saveAsTable(user_logs_2_table)"],"metadata":{},"outputs":[],"execution_count":32}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"name":"kkbox-read-data","notebookId":1002767001061388},"nbformat":4,"nbformat_minor":0}

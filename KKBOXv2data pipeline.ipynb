{"cells":[{"cell_type":"code","source":["#sqlDF = spark.sql(\"DROP TABLE transactions_date_fix\")\n#sqlDF = spark.sql(\"SELECT * , (transaction_date-20170300) as t_dat, (membership_expire_date-20170300) as e_dat FROM transactions_v2_csv \")\n\n#display(sqlDF)\n#sqlDF.write.format(\"parquet\").saveAsTable(\"transactions_date_fix_v2\")\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["sqlDF = spark.sql(\"SELECT *  FROM transactions_date_fix\")\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["sqlDF.count()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["sqlDF = spark.sql(\"SELECT msno, MAX(t_dat) as transaction_date, first(payment_method_id) as payment_method_id, AVG(payment_plan_days) as payment_plan_days,  first(plan_list_price) as plan_list_price, first(actual_amount_paid) as actual_amount_paid, first(is_auto_renew) as is_auto_renew, MAX(e_dat) as membership_expire_date, min(is_cancel) as is_cancel FROM transactions_date_fix GROUP BY msno \")\ndisplay(sqlDF)\n\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["sqlDF.count()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["sqlDF2 = spark.sql(\"SELECT msno, AVG(num_25) AS num_25_avg, AVG(num_50) AS num_50_avg, AVG(num_75) AS num_75_avg, AVG(num_985) AS num_985_avg, AVG(num_unq) AS num_unq_avg, AVG(total_secs) AS total_secs_avg FROM user_logs GROUP BY msno \")\ndisplay(sqlDF2)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["sqlDF2.count()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["sqlDF3 = spark.sql(\"SELECT * FROM train_v2_csv\")\nfinal = sqlDF.join(sqlDF2, 'msno', \"outer\")\nfinal2 = final.join(sqlDF3, 'msno', \"outer\")\nfinal3 = final2.na.drop(subset=[\"total_secs_avg\"])\nfinal4 = final3.na.drop(subset=[\"is_churn\"])\nfinal5 = final4.na.drop(subset=[\"transaction_date\"])\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(final4)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["sqlDF3.count()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["final3.count()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["final4.count()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(\n    inputCols=[\"num_25_avg\", \"num_50_avg\", \"num_75_avg\", \"num_unq_avg\", \"transaction_date\", \"payment_method_id\", \"payment_plan_days\", \"plan_list_price\",  \"actual_amount_paid\", \"is_auto_renew\", \"membership_expire_date\", \"total_secs_avg\"], outputCol=\"features\")\nfinal_df = assembler.transform(final5)\nfinal_df.select(\"features\").show(4, False)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.classification import LogisticRegression\n\n\n# Train a GBT model.\nmlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\", labelCol=\"is_churn\")\n\nlrModel = mlr.fit(final_df)\n\n\n# Make predictions.\n#predictions = model.transform(testData)\n\n# Select example rows to display.\n#predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(200)\n\n# Select (prediction, true label) and compute test error\n#evaluator = MulticlassClassificationEvaluator(\n#    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n#accuracy = evaluator.evaluate(predictions)\n#print(\"Test Error = %g\" % (1.0 - accuracy))\n\n#gbtModel = model.stages[2]\n#print(gbtModel)  # summary only"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#print(\"Test Error = %g\" % (1.0 - accuracy))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["print(\"Multinomial coefficients: \" + str(lrModel.coefficientMatrix))\nprint(\"Multinomial intercepts: \" + str(lrModel.interceptVector))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["sqlDFa = spark.sql(\"SELECT msno, MAX(t_dat) as transaction_date, first(payment_method_id) as payment_method_id, AVG(payment_plan_days) as payment_plan_days,  first(plan_list_price) as plan_list_price, first(actual_amount_paid) as actual_amount_paid, first(is_auto_renew) as is_auto_renew, MAX(e_dat) as membership_expire_date, min(is_cancel) as is_cancel FROM transactions_date_fix_v2 GROUP BY msno \")\ndisplay(sqlDFa)\n\nsqlDF2a = spark.sql(\"SELECT msno, AVG(num_25) AS num_25_avg, AVG(num_50) AS num_50_avg, AVG(num_75) AS num_75_avg, AVG(num_985) AS num_985_avg, AVG(num_unq) AS num_unq_avg, AVG(total_secs) AS total_secs_avg FROM user_logs_v2_csv GROUP BY msno \")\ndisplay(sqlDF2a)\n\nsqlDF3a = spark.sql(\"SELECT * FROM sample_submission_v2_csv\")\nfinala = sqlDFa.join(sqlDF2a, 'msno', \"outer\")\nfinal2a = finala.join(sqlDF3a, 'msno', \"outer\")\nfinal3a = final2a.na.drop(subset=[\"total_secs_avg\"])\nfinal4a = final3a.na.drop(subset=[\"is_churn\"])\nfinal5a = final4a.na.drop(subset=[\"transaction_date\"])\n"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["display(sqlDFa)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nassemblera = VectorAssembler(\n    inputCols=[\"num_25_avg\", \"num_50_avg\", \"num_75_avg\", \"num_unq_avg\", \"transaction_date\", \"payment_method_id\", \"payment_plan_days\", \"plan_list_price\",  \"actual_amount_paid\", \"is_auto_renew\", \"membership_expire_date\", \"total_secs_avg\"], outputCol=\"features\")\nfinal_dfa = assemblera.transform(final5a)\nfinal_dfa.select(\"features\").show(4, False)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=10).fit(final_df)\npredictionsa = lrModel.transform(final_dfa)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["display(predictionsa)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["predictionsa.count()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["predictionsa.registerTempTable(\"predictions\")\nmsnochurn = sql(\"SELECT msno, prediction FROM predictions\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["sqlDF11 = spark.sql(\"SELECT msno, is_churn FROM sample_submission_v2_csv\")\n\n\ndisplay(sqlDF11)\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["display(msnochurn)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["EOLL = msnochurn.join(sqlDF11, 'msno', \"outer\")\ndisplay(EOLL)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["#spark.sql(\"DROP TABLE output\")\nEOLL.registerTempTable(\"done_tabb\")\n\ntofile = sql(\"SELECT msno, COALESCE(is_churn, 0) + COALESCE(prediction, 0) as is_churn FROM done_tabb\")\n"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["display(tofile)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["\n\ndone.write.format(\"parquet\").saveAsTable(\"outbounddata\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["tocsv = spark.sql(\"SELECT msno, COALESCE(is_churn, 0) + COALESCE(prediction, 0) as is_churn FROM output\")\ndisplay(tocsv)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["tocsv.count()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["tocsv.write.csv('mycsv.csv')"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":35}],"metadata":{"name":"KKBOXv2data (1)","notebookId":1485883831445202},"nbformat":4,"nbformat_minor":0}

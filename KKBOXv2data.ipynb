{"cells":[{"cell_type":"code","source":["#sqlDF = spark.sql(\"DROP TABLE transactions_date_fix\")\n\n\nsqlDF = spark.sql(\"SELECT * , (transaction_date-20170300) as t_dat, (membership_expire_date-20170300) as e_dat FROM transactions_v2_csv \")\ndisplay(sqlDF)\nsqlDF.write.format(\"parquet\").saveAsTable(\"transactions_date_fix_v2\")\n\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["sqlDF = spark.sql(\"SELECT *  FROM transactions_date_fix\")\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["sqlDF.count()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["sqlDF = spark.sql(\"SELECT msno, MAX(t_dat) as transaction_date, first(payment_method_id) as payment_method_id, AVG(payment_plan_days) as payment_plan_days,  first(plan_list_price) as plan_list_price, first(actual_amount_paid) as actual_amount_paid, first(is_auto_renew) as is_auto_renew, MAX(e_dat) as membership_expire_date, min(is_cancel) as is_cancel FROM transactions_date_fix GROUP BY msno \")\ndisplay(sqlDF)\n\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["sqlDF.count()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["sqlDF2 = spark.sql(\"SELECT msno, AVG(num_25) AS num_25_avg, AVG(num_50) AS num_50_avg, AVG(num_75) AS num_75_avg, AVG(num_985) AS num_985_avg, AVG(num_unq) AS num_unq_avg, AVG(total_secs) AS total_secs_avg FROM user_logs GROUP BY msno \")\ndisplay(sqlDF2)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["sqlDF2.count()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["sqlDF3 = spark.sql(\"SELECT * FROM train_v2_csv\")\nfinal = sqlDF.join(sqlDF2, 'msno', \"outer\")\nfinal2 = final.join(sqlDF3, 'msno', \"outer\")\nfinal3 = final2.na.drop(subset=[\"total_secs_avg\"])\nfinal4 = final3.na.drop(subset=[\"is_churn\"])\nfinal5 = final4.na.drop(subset=[\"transaction_date\"])\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(final4)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["sqlDF3.count()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["final3.count()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["final4.count()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(\n    inputCols=[\"num_25_avg\", \"num_50_avg\", \"num_75_avg\", \"num_unq_avg\",  \"payment_method_id\", \"payment_plan_days\", \"plan_list_price\",  \"actual_amount_paid\", \"is_auto_renew\", \"membership_expire_date\", \"transaction_date\", \"is_cancel\",  \"total_secs_avg\"], outputCol=\"features\")\nfinal_df = assembler.transform(final5)\nfinal_df.select(\"features\").show(4, False)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n\nlabelIndexer = StringIndexer(inputCol=\"is_churn\", outputCol=\"indexedLabel\").fit(final_df)\nfeatureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=10).fit(final_df)\n  # Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = final_df.randomSplit([0.70, 0.30])\n# Train a GBT model.\ngbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n# Chain indexers and GBT in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n# Make predictions.\npredictions = model.transform(testData)\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(200)\n#Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\ngbtModel = model.stages[2]\nprint(gbtModel)  # summary only"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.mllib.evaluation import BinaryClassificationMetrics\n\npredictionAndLabelTest = predictions.select(['prediction','indexedLabel']).rdd.map(lambda x: ( float(x[0]) , float(x[1]) ) )\n#predictionAndLabelTrain = trainingData.select(['prediction','is_churn']).rdd.map(lambda x: ( float(x[0]) , float(x[1]) ) )\n\nmetricsTest = BinaryClassificationMetrics(predictionAndLabelTest)\n#metricsTrain = BinaryClassificationMetrics(predictionAndLabelTrain)\nprint 'Area under precision: test = %s'  % metricsTest.areaUnderPR\nprint 'Area under       ROC: test = %s' % metricsTest.areaUnderROC"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["print(\"Test Error = %g\" % ((1.0 - accuracy)*100))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["sqlDFa = spark.sql(\"SELECT msno, MAX(t_dat) as transaction_date, first(payment_method_id) as payment_method_id, AVG(payment_plan_days) as payment_plan_days,  first(plan_list_price) as plan_list_price, first(actual_amount_paid) as actual_amount_paid, first(is_auto_renew) as is_auto_renew, MAX(e_dat) as membership_expire_date, max(is_cancel) as is_cancel FROM transactions_date_fix_v2 GROUP BY msno \")\ndisplay(sqlDFa)\n\nsqlDF2a = spark.sql(\"SELECT msno, AVG(num_25) AS num_25_avg, AVG(num_50) AS num_50_avg, AVG(num_75) AS num_75_avg, AVG(num_985) AS num_985_avg, AVG(num_unq) AS num_unq_avg, AVG(total_secs) AS total_secs_avg FROM user_logs_v2_csv GROUP BY msno \")\ndisplay(sqlDF2a)\n\nsqlDF3a = spark.sql(\"SELECT * FROM sample_submission_v2_csv\")\nfinala = sqlDFa.join(sqlDF2a, 'msno', \"outer\")\nfinal2a = finala.join(sqlDF3a, 'msno', \"outer\")\nfinal3a = final2a.na.drop(subset=[\"total_secs_avg\"])\nfinal4a = final3a.na.drop(subset=[\"is_churn\"])\nfinal5a = final4a.na.drop(subset=[\"transaction_date\"])\n"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["display(sqlDFa)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nassemblera = VectorAssembler(\n    inputCols=[\"num_25_avg\", \"num_50_avg\", \"num_75_avg\", \"num_unq_avg\",\"payment_method_id\", \"payment_plan_days\", \"plan_list_price\",  \"actual_amount_paid\", \"is_auto_renew\", \"membership_expire_date\", \"total_secs_avg\"], outputCol=\"features\")\nfinal_dfa = assemblera.transform(final5a)\nfinal_dfa.select(\"features\").show(4, False)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["#featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=10).fit(final_df)\npredictionsa = model.transform(final_dfa)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(predictionsa)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["predictionsa.count()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["predictionsa.registerTempTable(\"predictions\")\nmsnochurn = sql(\"SELECT msno, prediction FROM predictions\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["\nfrom pyspark.ml.feature import VectorAssembler\n\nassemblera = VectorAssembler(\n    inputCols=[\"num_25_avg\", \"num_50_avg\", \"num_75_avg\", \"num_unq_avg\",\"payment_method_id\", \"payment_plan_days\", \"plan_list_price\",  \"actual_amount_paid\", \"is_auto_renew\", \"membership_expire_date\", \"total_secs_avg\"], outputCol=\"features\")\nfinal_dfa = assemblera.transform(final5a)\nfinal_dfa.select(\"features\").show(4, False)\n\npredictionsa = model.transform(final_dfa)\n\ndisplay(predictionsa)\npredictionsa.registerTempTable(\"predictions\")\nmsnochurn = sql(\"SELECT msno, prediction FROM predictions\")\nsqlDF11 = spark.sql(\"SELECT msno, is_churn FROM sample_submission_v2_csv\")\nEOLL = msnochurn.join(sqlDF11, 'msno', \"outer\")\ndisplay(EOLL)\nEOLL.registerTempTable(\"done_tabb\")\n\ntofile = sql(\"SELECT msno, COALESCE(is_churn, 0) + COALESCE(prediction, 0) as is_churn FROM done_tabb\")\ndisplay(tofile)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["display(msnochurn)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["EOLL = msnochurn.join(sqlDF11, 'msno', \"outer\")\ndisplay(EOLL)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["#spark.sql(\"DROP TABLE output\")\nEOLL.registerTempTable(\"done_tabb\")\n\ntofile = sql(\"SELECT msno, COALESCE(is_churn, 0) + COALESCE(prediction, 0) as is_churn FROM done_tabb\")\n"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["tofile.count()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["display(tofile)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["tofile.groupBy('is_churn').count().show()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["tocsv = spark.sql(\"SELECT msno, COALESCE(is_churn, 0) + COALESCE(prediction, 0) as is_churn FROM output\")\ndisplay(tocsv)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["tocsv.count()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["tocsv.write.csv('mycsv.csv')"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":36}],"metadata":{"name":"KKBOXv2data","notebookId":3401685903177248},"nbformat":4,"nbformat_minor":0}
